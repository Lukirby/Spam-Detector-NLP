{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detector - NLP Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus as corpus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import time\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: (5172, 3002)\n",
      "Index Column Email No.\n",
      "Label Column Prediction\n"
     ]
    }
   ],
   "source": [
    "path = \"dataset/emails.csv\"\n",
    "\n",
    "dataset = pd.read_csv(path)\n",
    "\n",
    "print('Dataset Size:', dataset.shape)\n",
    "\n",
    "print(\"Index Column\",dataset.columns[0])\n",
    "\n",
    "print(\"Label Column\",dataset.columns[-1])\n",
    "\n",
    "X = dataset.drop([dataset.columns[0],dataset.columns[-1]],axis=1)\n",
    "Y = dataset[dataset.columns[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preoprocessing\n",
    "\n",
    "The dataset has been splitted in two components: **Train Set** and **Test Set**.\n",
    "The train set will be used for the training the model, while the test set, only to measure the \n",
    "efficancy of the model.\n",
    "\n",
    "I applied **shuffling** to the split to avoid ordered bias on data, and **stratification** \n",
    "so that the train and test set have the same ratio of spam and non spam emails, \n",
    "improving the vailidity of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Train Size (4137, 3000)\n",
      "Data Test Size (1035, 3000)\n",
      "Label Train Size (4137,)\n",
      "Label Test Size (1035,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def dataset_split(dataset: pd.DataFrame,train_size:float|None) -> tuple[pd.DataFrame,pd.DataFrame,pd.DataFrame,pd.DataFrame]:\n",
    "    D = dataset[dataset.columns.drop('Email No.')]\n",
    "    X = D[D.columns.drop('Prediction')]\n",
    "    Y = D['Prediction']#.apply(lambda x: 'no-spam' if x == 0 else 'spam')\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,train_size=train_size,shuffle=True,stratify=Y)\n",
    "    return X_train,X_test,Y_train,Y_test\n",
    "\n",
    "train_size = 0.8\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = dataset_split(dataset=dataset,train_size=train_size)\n",
    "\n",
    "print('Data Train Size',X_train.shape)\n",
    "print('Data Test Size',X_test.shape)\n",
    "print('Label Train Size',Y_train.shape)\n",
    "print('Label Test Size',Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Dataset\n",
    "\n",
    "I created a different dataset from the original, where I removed all the \n",
    "**english stopwords**, this to test if the there is a kind of improvement, \n",
    "to the performance, even if it is unaspected to the Naive Bayes Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords:  179\n",
      "['i', 'me', 'my', 'myself', 'we']\n",
      "Features:  2866\n"
     ]
    }
   ],
   "source": [
    "stopwords = corpus.stopwords.words('english')\n",
    "print(\"Stopwords: \",len(stopwords))\n",
    "print(stopwords[:5])\n",
    "inside_stopwords = X_train.columns.difference(stopwords)\n",
    "X_train_stop = X_train[inside_stopwords]\n",
    "X_test_stop = X_test[inside_stopwords]\n",
    "print(\"Features: \",len(X_train_stop.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information Dataset\n",
    "\n",
    "Another variation of the dataset is to remove all the words that do not bring enough \n",
    "information to the classification of the train set.\n",
    "\n",
    "The **Mutual Information** between the single features and the labels, with the following formula:\n",
    "$$\n",
    "    I(\\text{word},\\textbf{labels}) = p(\\text{word},\\text{spam}) \\log\\left(\\frac{p(\\text{word},\\text{spam})}{p(\\text{word})p(\\text{spam})}\\right) +  \n",
    "    p(\\text{word},\\text{nospam}) \\log\\left(\\frac{p(\\text{word},\\text{nospam})}{p(\\text{word})p(\\text{nospam})}\\right)\n",
    "$$\n",
    "\n",
    "After retrieving all the information value for each featuere, i discard those below a fixed threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  670\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "mi_threshold = 0.01\n",
    "mi = mutual_info_classif(X_train,Y_train)\n",
    "\n",
    "#Vector of mutual information with index = word feature \n",
    "mi_series = pd.Series(mi, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "#Select the feature which have mutual information highter than the threshold\n",
    "\n",
    "selected_words = mi_series[mi_series>mi_threshold].index\n",
    "\n",
    "X_train_mi = X_train[selected_words]\n",
    "X_test_mi = X_test[selected_words]\n",
    "print(\"Features: \",len(X_train_mi.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.probability import LidstoneProbDist\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **NLTK** library use a different data structures for input insted of matrix or dataframe.\n",
    "It use the a list of dictonaries to performe training, predicting and metrics calculation.\n",
    "Therefore i created some function that cast dataframe to list of dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labeled_featureset(X:pd.DataFrame,Y: pd.DataFrame) -> list:\n",
    "    labeled_featuresets = [\n",
    "        (row[1].to_dict(), str(label)) \n",
    "        for row, label in zip(X.iterrows(), Y)\n",
    "    ]\n",
    "    return labeled_featuresets\n",
    "\n",
    "def make_featureset(X: pd.DataFrame) -> list:\n",
    "    featuresets = [\n",
    "        row[1].to_dict() \n",
    "        for row in X.iterrows()\n",
    "    ]\n",
    "    return featuresets\n",
    "\n",
    "def make_label_list(Y: pd.DataFrame) -> list:\n",
    "    return [str(label) for label in Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "The **Naive Bayes Model** classify the data using the bayes formula, the parameters are estimated in this way:\n",
    "$$\n",
    "    p(\\text{label}) = \\frac{count(\\text{label})}{count(\\text{allLabels})} \\\\ \\\\\n",
    "    p(\\text{word}|\\text{label}) = \\frac{count(\\text{word},\\text{label}) + \\gamma}{count(\\text{word},\\text{allLabels}) + \\gamma |V|}\n",
    "$$\n",
    "Where $V$ is the number of total words used for the training.\n",
    "In this case the parameter $\\mathbf{\\gamma}$ can be tuned for better performance.\n",
    "\n",
    "But the current implementation of the class `NaiveBayesClassifier` do not allows \n",
    "edit to this parameter, it take as estimator a particular instance of the above formula\n",
    "(`LidstoneProbDist` object) with $\\gamma=0.5$,\n",
    "in this case i override the estimator using a function that return an estimator that have\n",
    "fixed gamma parameter of our choise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NBC(X_train:pd.DataFrame,Y_train: pd.DataFrame,gamma:float) -> NaiveBayesClassifier:\n",
    "    XY_train = make_labeled_featureset(X_train,Y_train)\n",
    "    #override of the estimator with gamma of our choise\n",
    "    prob_estimator = lambda freqdist, bins=None : LidstoneProbDist(freqdist,gamma,bins)\n",
    "    nbc = NaiveBayesClassifier.train(XY_train,estimator=prob_estimator)\n",
    "    return nbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_NBC(X_test:pd.DataFrame,nbc:NaiveBayesClassifier) -> list:\n",
    "    X_test = make_featureset(X_test)\n",
    "    return nbc.classify_many(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation e Fine Tuning\n",
    "\n",
    "To tune the model on the best $\\gamma$ parameter, I use the the **K-Fold Cross Validation** method, \n",
    "which mean that the training set will be splitted in $k$ segments, and the model will be trained \n",
    "$k$ times on the $k-1$ segments, and the $k$th will be used as test set.\n",
    "\n",
    "For the evaluation will be used the mean **f1-score** of all $k$ execution, for each $\\gamma$ in input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def cross_validation_fine_tuning(X_train:pd.DataFrame,Y_train:pd.DataFrame,Gammas,n_split:int,verbose:bool=False) -> dict:\n",
    "\n",
    "    list_measures = {gamma: [] for gamma in Gammas}\n",
    "    mean_measures = {}\n",
    "\n",
    "    if n_split > 1:\n",
    "        for gamma in Gammas:\n",
    "            skf = StratifiedKFold(n_splits=n_split,shuffle=True)\n",
    "            skf.get_n_splits(X_train,Y_train)\n",
    "            k_folds = skf.split(X_train,Y_train)\n",
    "            # k_fold containt a list of indexes that will rappresent the kth-fold for test, \n",
    "            # and the others will be use for training.\n",
    "            for j,(train_index, val_index) in enumerate(k_folds):\n",
    "                X_train_k = X_train.iloc[train_index]\n",
    "                Y_train_k = Y_train.iloc[train_index]\n",
    "                X_val_k = X_train.iloc[val_index]\n",
    "                Y_val_k = Y_train.iloc[val_index]\n",
    "\n",
    "                nbc = train_NBC(X_train_k,Y_train_k,gamma)\n",
    "                Y_pred = classify_NBC(X_val_k,nbc)\n",
    "                Y_pred = [int(label) for label in Y_pred]\n",
    "                score = f1_score(Y_pred,Y_val_k)\n",
    "                if (verbose):\n",
    "                    print(f\"Gamma {gamma} on {j}-fold: {score}\")\n",
    "                list_measures[gamma].append(score)\n",
    "\n",
    "            mean_measures[gamma] = np.mean(list_measures[gamma])\n",
    "\n",
    "    else:\n",
    "        X_train_k, X_val_k,Y_train_k,Y_val_k = train_test_split(X_train,Y_train,shuffle=True,stratify=Y_train,train_size=0.9)\n",
    "        for gamma in Gammas:\n",
    "            nbc = train_NBC(X_train_k,Y_train_k,gamma)\n",
    "            Y_pred = classify_NBC(X_val_k,nbc)\n",
    "            Y_pred = [int(label) for label in Y_pred]\n",
    "            score = f1_score(Y_pred,Y_val_k)\n",
    "            list_measures[gamma].append(score)\n",
    "            mean_measures[gamma] = np.mean(list_measures[gamma])\n",
    "\n",
    "    return mean_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply retrive the gamma assosieted with the highest mean f1 score on the k executions\n",
    "def get_best_gamma(eval_measures:dict) -> int:\n",
    "    best_gamma = max(eval_measures,key=eval_measures.get)\n",
    "    print(\"Best Gamma: \",best_gamma)\n",
    "    print(\"Mean F1: \",eval_measures[best_gamma])\n",
    "    return best_gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "For the final evaluation after choose the best $\\gamma$, will be used \n",
    "the **Confusion Matrix** of NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(X_test:pd.DataFrame,Y_test:pd.DataFrame,nbc:NaiveBayesClassifier) -> ConfusionMatrix:\n",
    "    Y_test = make_label_list(Y_test)\n",
    "    Y_pred = classify_NBC(X_test,nbc)\n",
    "    return ConfusionMatrix(Y_pred,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gamma Parameters\n",
    "\n",
    "Usually the parameter $\\gamma$ range from $0$ to $1$, after a first search\n",
    "in that range, I locateted that the best parameter is lower than $0.25$,\n",
    "so i performed a fine tuning of $\\gamma$ in range $(0,0.25)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gammas = [0,0.05,0.10,0.15,0.20,0.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Time:  202.77839255332947  seconds\n",
      "Best Gamma:  0.05\n",
      "Mean F1:  0.802905374411449\n",
      "Tag | Prec.  | Recall | F-measure\n",
      "----+--------+--------+-----------\n",
      "  0 | 0.9442 | 0.8990 | 0.9210\n",
      "  1 | 0.7400 | 0.8441 | 0.7886\n",
      "\n",
      "Training Time:  5.747609376907349  seconds\n",
      "Classification Time:  2.6166563034057617  seconds\n"
     ]
    }
   ],
   "source": [
    "start_cv = time.time()\n",
    "mean_f1_scores = cross_validation_fine_tuning(X_train,Y_train,Gammas,n_split=5)\n",
    "end_cv = time.time()\n",
    "\n",
    "print(\"Cross Validation Time: \",end_cv-start_cv,\" seconds\")\n",
    "\n",
    "best_gamma = get_best_gamma(mean_f1_scores)\n",
    "\n",
    "start_train = time.time()  \n",
    "nbc_complete = train_NBC(X_train,Y_train,best_gamma)\n",
    "end_train = time.time()  \n",
    "\n",
    "start_predict = time.time()\n",
    "cs_complete = eval_model(X_test,Y_test,nbc_complete)\n",
    "end_predict = time.time()\n",
    "\n",
    "print(cs_complete.evaluate())\n",
    "print(\"Training Time: \",end_train-start_train,\" seconds\")\n",
    "print(\"Classification Time: \",end_predict-start_predict,\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Time:  195.52402806282043  seconds\n",
      "Best Gamma:  0.05\n",
      "Mean F1:  0.8026579917887823\n",
      "Tag | Prec.  | Recall | F-measure\n",
      "----+--------+--------+-----------\n",
      "  0 | 0.9442 | 0.8978 | 0.9204\n",
      "  1 | 0.7367 | 0.8435 | 0.7865\n",
      "\n",
      "Training Time:  5.916151285171509  seconds\n",
      "Classification Time:  2.799798011779785  seconds\n"
     ]
    }
   ],
   "source": [
    "start_cv = time.time()\n",
    "mean_f1_scores = cross_validation_fine_tuning(X_train_stop,Y_train,Gammas,n_split=5)\n",
    "end_cv = time.time()\n",
    "\n",
    "print(\"Cross Validation Time: \",end_cv-start_cv,\" seconds\")\n",
    "\n",
    "best_gamma = get_best_gamma(mean_f1_scores)\n",
    "\n",
    "start_train = time.time()  \n",
    "nbc_stop = train_NBC(X_train_stop,Y_train,best_gamma)\n",
    "end_train = time.time()  \n",
    "\n",
    "start_predict = time.time()\n",
    "cs_stop = eval_model(X_test_stop,Y_test,nbc_stop)\n",
    "end_predict = time.time()\n",
    "\n",
    "print(cs_stop.evaluate())\n",
    "print(\"Training Time: \",end_train-start_train,\" seconds\")\n",
    "print(\"Classification Time: \",end_predict-start_predict,\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset with Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Time:  44.99998927116394  seconds\n",
      "Best Gamma:  0.05\n",
      "Mean F1:  0.8267710652323135\n",
      "Tag | Prec.  | Recall | F-measure\n",
      "----+--------+--------+-----------\n",
      "  0 | 0.9442 | 0.9144 | 0.9290\n",
      "  1 | 0.7833 | 0.8514 | 0.8160\n",
      "\n",
      "Training Time:  1.272050380706787  seconds\n",
      "Classification Time:  0.6166384220123291  seconds\n"
     ]
    }
   ],
   "source": [
    "start_cv = time.time()\n",
    "mean_f1_scores = cross_validation_fine_tuning(X_train_mi,Y_train,Gammas,n_split=5)\n",
    "end_cv = time.time()\n",
    "\n",
    "print(\"Cross Validation Time: \",end_cv-start_cv,\" seconds\")\n",
    "\n",
    "best_gamma = get_best_gamma(mean_f1_scores)\n",
    "\n",
    "start_train = time.time()  \n",
    "nbc_mi = train_NBC(X_train_mi,Y_train,best_gamma)\n",
    "end_train = time.time()  \n",
    "\n",
    "start_predict = time.time()\n",
    "cs_mi = eval_model(X_test_mi,Y_test,nbc_mi)\n",
    "end_predict = time.time()\n",
    "\n",
    "print(cs_mi.evaluate())\n",
    "print(\"Training Time: \",end_train-start_train,\" seconds\")\n",
    "print(\"Classification Time: \",end_predict-start_predict,\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Apparently the best gamma for all the dataset is $\\gamma=0.05$ \n",
    "and the dataset reduced using mutual information, perform better in classification.\n",
    "We can also observe, what words weigth more in the classification procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Complete:\n",
      "Most Informative Features\n",
      "               forwarded = 1                   0 : 1      =    214.9 : 1.0\n",
      "                     sex = 1                   1 : 0      =    167.9 : 1.0\n",
      "                     biz = 1                   1 : 0      =    116.6 : 1.0\n",
      "                      rx = 1                   1 : 0      =    112.0 : 1.0\n",
      "                     hpl = 1                   0 : 1      =    109.0 : 1.0\n",
      "                    spam = 1                   1 : 0      =    100.3 : 1.0\n",
      "                  weight = 1                   1 : 0      =     98.0 : 1.0\n",
      "                     iit = 1                   1 : 0      =     88.7 : 1.0\n",
      "                   cheap = 1                   1 : 0      =     81.2 : 1.0\n",
      "                   sleep = 1                   1 : 0      =     79.4 : 1.0\n",
      "Dataset StopWord:\n",
      "Most Informative Features\n",
      "               forwarded = 1                   0 : 1      =    214.9 : 1.0\n",
      "                     sex = 1                   1 : 0      =    167.9 : 1.0\n",
      "                     biz = 1                   1 : 0      =    116.6 : 1.0\n",
      "                      rx = 1                   1 : 0      =    112.0 : 1.0\n",
      "                     hpl = 1                   0 : 1      =    109.0 : 1.0\n",
      "                    spam = 1                   1 : 0      =    100.3 : 1.0\n",
      "                  weight = 1                   1 : 0      =     98.0 : 1.0\n",
      "                     iit = 1                   1 : 0      =     88.7 : 1.0\n",
      "                   cheap = 1                   1 : 0      =     81.2 : 1.0\n",
      "                   sleep = 1                   1 : 0      =     79.4 : 1.0\n",
      "Dataset Mutual Information:\n",
      "Most Informative Features\n",
      "               forwarded = 1                   0 : 1      =    225.5 : 1.0\n",
      "                     sex = 1                   1 : 0      =    176.2 : 1.0\n",
      "                     biz = 1                   1 : 0      =    122.4 : 1.0\n",
      "                      rx = 1                   1 : 0      =    117.5 : 1.0\n",
      "                     hpl = 1                   0 : 1      =    114.4 : 1.0\n",
      "                  weight = 1                   1 : 0      =    102.8 : 1.0\n",
      "                     iit = 1                   1 : 0      =     93.0 : 1.0\n",
      "                   cheap = 1                   1 : 0      =     83.2 : 1.0\n",
      "             legislation = 1                   1 : 0      =     75.9 : 1.0\n",
      "                    duty = 2                   1 : 0      =     68.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Complete:\")\n",
    "nbc_complete.show_most_informative_features()\n",
    "print(\"Dataset StopWord:\")\n",
    "nbc_stop.show_most_informative_features()\n",
    "print(\"Dataset Mutual Information:\")\n",
    "nbc_mi.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the most relevant words are `forwarded`, `sex` and `biz`, in particular \n",
    "-  `forwarded` is associeted to **non-spam** email with a ratio 215 to 1\n",
    "-  `sex` is associeted to **spam** email with a ratio 168 to 1\n",
    "-  `biz` is associeted to **spam** email with a ratio 116 to 1\n",
    "\n",
    "Also these words, have more weigth in the mutual information dataset, since most of the \n",
    "other words, are been excludeted since they bring low information.\n",
    "This dataset perform better and it significantly fasten the train and prediction of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natural-language-processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
